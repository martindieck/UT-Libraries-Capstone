{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "   ---------------------------------------- 0.0/250.0 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/250.0 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 250.0/250.0 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data saved to C:\\Users\\angel\\OneDrive\\Desktop\\Capstone\\AAA_Proj_db_20231019-MSITM_Architech_curated_output.xlsx\n",
      "Unique data saved to C:\\Users\\angel\\OneDrive\\Desktop\\Capstone\\Unique_AAA_Proj_db_20231019-MSITM_Architech.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"\n",
    "    Clean the name by removing unwanted characters and dates.\n",
    "    \"\"\"\n",
    "    name = re.sub('[^\\x00-\\x7F]+', '', name)  # Remove non-ASCII characters\n",
    "    name = re.sub('\\(\\d{4}-\\d{4}\\)|\\(\\d{4}\\)', '', name)  # Remove dates\n",
    "    return name.strip()\n",
    "\n",
    "def process_entry_v5(entry):\n",
    "    \"\"\"\n",
    "    Process each entry with enhanced rules for corporate identification and name cleaning.\n",
    "    Converts any non-string input into a string to handle NaN values.\n",
    "    \"\"\"\n",
    "    # Convert entry to string to handle NaN values gracefully\n",
    "    entry = str(entry)\n",
    "    \n",
    "    if 'unidentified' in entry.lower():\n",
    "        return [(\"<flag>\", \"<flag>\", \"<flag>\")]\n",
    "    \n",
    "    #corporate_keywords = [\n",
    "    #    'Group', 'Company', 'Corporation', 'Incorporated', 'Inc.', 'LLC', 'Ltd.',\n",
    "    #    'and', '&', 'Architects', 'Architecture', 'Studio', 'Associates', 'Bureau',\n",
    "    #    'Office of Supervising Architect', 'Engineering', 'Construction', 'Consultants',\n",
    "    #    'Design', 'Partnership', 'Firm', 'PLLC', 'Office of',\n",
    "    #    'Skidmore Owings Merrill', 'Venturi Rauch & Scott Brown',\n",
    "    #]\n",
    "    corporate_keywords = [\n",
    "    'Group', 'Company', 'Corporation', 'Incorporated', 'Inc.', 'LLC', 'Ltd.',\n",
    "    'and', '&', 'Architects', 'Architecture', 'Studio', 'Associates', 'Bureau',\n",
    "    'Office of Supervising Architect', 'Engineering', 'Construction', 'Consultants',\n",
    "    'Design', 'Partnership', 'Firm', 'PLLC', 'Office of',\n",
    "    'Skidmore Owings Merrill', 'Venturi Rauch & Scott Brown', 'architectural firm',\n",
    "    'architects', 'architecture', 'engineering', 'construction', 'design studio'\n",
    "    ]\n",
    "\n",
    "    \n",
    "    entry = clean_name(entry)\n",
    "    primary_entities = [e.strip() for e in entry.split(';')]\n",
    "    processed_entries = []\n",
    "    \n",
    "    for entity in primary_entities:\n",
    "        if '|' in entity:\n",
    "            name, role = [part.strip() for part in entity.split('|', 1)]\n",
    "        else:\n",
    "            name, role = entity, 'contributor'\n",
    "        \n",
    "        entity_type = 'corporate' if any(keyword.lower() in name.lower() for keyword in corporate_keywords) else 'person'\n",
    "        processed_entries.append((name, role, entity_type))\n",
    "    \n",
    "    return processed_entries\n",
    "\n",
    "def expand_entries(row):\n",
    "    \"\"\"\n",
    "    Expand processed entries into separate columns within the same row.\n",
    "    \"\"\"\n",
    "    expanded_data = {}\n",
    "    for i, (name, role, entity_type) in enumerate(row['Processed'], start=1):\n",
    "        expanded_data[f'Primary archt/firm[{i}][Name]'] = name\n",
    "        expanded_data[f'Primary archt/firm[{i}][Role]'] = role\n",
    "        expanded_data[f'Primary archt/firm[{i}][Type]'] = entity_type\n",
    "    return pd.Series(expanded_data)\n",
    "\n",
    "def process_and_save_excel(input_excel_path, output_excel_path, unique_output_path):\n",
    "    \"\"\"\n",
    "    Load the input Excel, process it with the latest rules, save the outputs,\n",
    "    and also save a second output with unique values for specified columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(input_excel_path)\n",
    "    \n",
    "    # Apply the 'process_entry_v5' function to the 'Primary archt/firm' column\n",
    "    df['Processed'] = df['Primary archt/firm'].apply(process_entry_v5)\n",
    "    \n",
    "    # Expand processed entries\n",
    "    expanded_df = df.apply(expand_entries, axis=1)\n",
    "    \n",
    "    # Concatenate the original dataframe with the expanded dataframe\n",
    "    result_df = pd.concat([df, expanded_df], axis=1).drop(columns=['Processed'])\n",
    "    \n",
    "    # Save the result to an Excel file\n",
    "    result_df.to_excel(output_excel_path, index=False)\n",
    "    print(f\"All data saved to {output_excel_path}\")\n",
    "\n",
    "    # Select columns for the unique data output\n",
    "    columns_to_keep = [ 'Primary archt/firm'] + \\\n",
    "                      [col for col in result_df.columns if col.startswith('Primary archt/firm[')]\n",
    "    unique_df = result_df[columns_to_keep].drop_duplicates()\n",
    "    \n",
    "    # Save the unique entries to another Excel file\n",
    "    unique_df.to_excel(unique_output_path, index=False)\n",
    "    print(f\"Unique data saved to {unique_output_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_excel_path = 'C:\\\\Users\\\\angel\\\\OneDrive\\\\Desktop\\\\Capstone\\\\AAA_Proj_db_20231019-MSITM_Architech_curated.xlsx'\n",
    "output_excel_path = 'C:\\\\Users\\\\angel\\\\OneDrive\\\\Desktop\\\\Capstone\\\\AAA_Proj_db_20231019-MSITM_Architech_curated_output.xlsx'\n",
    "unique_output_path = 'C:\\\\Users\\\\angel\\\\OneDrive\\\\Desktop\\\\Capstone\\\\Unique_AAA_Proj_db_20231019-MSITM_Architech.xlsx'\n",
    "process_and_save_excel(input_excel_path, output_excel_path, unique_output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"\n",
    "    Clean the name by removing unwanted characters and dates.\n",
    "    \"\"\"\n",
    "    name = re.sub('[^\\x00-\\x7F]+', '', name)  # Remove non-ASCII characters\n",
    "    name = re.sub('\\(\\d{4}-\\d{4}\\)|\\(\\d{4}\\)', '', name)  # Remove dates\n",
    "    return name.strip()\n",
    "\n",
    "def process_entry_v5(entry):\n",
    "    \"\"\"\n",
    "    Process each entry with enhanced rules for corporate identification and name cleaning.\n",
    "    Converts any non-string input into a string to handle NaN values.\n",
    "    \"\"\"\n",
    "    entry = str(entry)\n",
    "    \n",
    "    if 'unidentified' in entry.lower():\n",
    "        return [(\"\", \"\", \"\")]\n",
    "    \n",
    "    corporate_keywords = [\n",
    "        'Group', 'Company', 'Corporation', 'Incorporated', 'Inc.', 'LLC', 'Ltd.',\n",
    "        'and', '&', 'Architects', 'Architecture', 'Studio', 'Associates', 'Bureau',\n",
    "        'Office of Supervising Architect', 'Engineering', 'Construction', 'Consultants',\n",
    "        'Design', 'Partnership', 'Firm', 'PLLC', 'Office of',\n",
    "        'Skidmore Owings Merrill', 'Venturi Rauch & Scott Brown', 'architectural firm',\n",
    "        'architects', 'architecture', 'engineering', 'construction', 'design studio'\n",
    "    ]\n",
    "    \n",
    "    entry = clean_name(entry)\n",
    "    primary_entities = [e.strip() for e in entry.split(';')]\n",
    "    processed_entries = []\n",
    "    \n",
    "    for entity in primary_entities:\n",
    "        if '|' in entity:\n",
    "            name, role = [part.strip() for part in entity.split('|', 1)]\n",
    "        else:\n",
    "            name, role = entity, 'contributor'\n",
    "        \n",
    "        entity_type = 'corporate' if any(keyword.lower() in name.lower() for keyword in corporate_keywords) else 'person'\n",
    "        processed_entries.append((name, role, entity_type))\n",
    "    \n",
    "    return processed_entries\n",
    "\n",
    "def expand_entries(row):\n",
    "    \"\"\"\n",
    "    Expand processed entries into separate columns within the same row.\n",
    "    \"\"\"\n",
    "    expanded_data = {}\n",
    "    for i, (name, role, entity_type) in enumerate(row['Processed'], start=1):\n",
    "        expanded_data[f'Primary archt/firm[{i}][Name]'] = name\n",
    "        expanded_data[f'Primary archt/firm[{i}][Role]'] = role\n",
    "        expanded_data[f'Primary archt/firm[{i}][Type]'] = entity_type\n",
    "    return pd.Series(expanded_data)\n",
    "\n",
    "def process_and_save_csv(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Load the input CSV, process it with the latest rules, and save the outputs.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    df['Processed'] = df['Primary archt/firm'].apply(process_entry_v5)\n",
    "    expanded_df = df.apply(expand_entries, axis=1)\n",
    "    result_df = pd.concat([df, expanded_df], axis=1).drop(columns=['Processed'])\n",
    "    \n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"All data saved to {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = 'C:\\\\Users\\\\angel\\\\OneDrive\\\\Desktop\\\\Capstone\\\\AAA_Proj_db_20231019-MSITM_Architech_curated.csv'\n",
    "output_csv_path = 'C:\\\\Users\\\\angel\\\\OneDrive\\\\Desktop\\\\Capstone\\\\AAA_Proj_db_20231019-MSITM_Architech_curated_output.csv'\n",
    "process_and_save_csv(input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_41236\\2730285286.py:66: DtypeWarning: Columns (35,43,47,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data saved to C:\\Users\\angel\\OneDrive\\Desktop\\Capstone\\Primary_archtfirm\\AAA_Proj_db_20231019-MSITM_Architech_curated_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"\n",
    "    Clean the name by removing unwanted characters and dates.\n",
    "    \"\"\"\n",
    "    name = re.sub('[^\\x00-\\x7F]+', '', name)  # Remove non-ASCII characters\n",
    "    name = re.sub('\\(\\d{4}-\\d{4}\\)|\\(\\d{4}\\)', '', name)  # Remove dates\n",
    "    return name.strip()\n",
    "\n",
    "def process_entry_v5(entry):\n",
    "    \"\"\"\n",
    "    Process each entry with enhanced rules for corporate identification and name cleaning.\n",
    "    Converts any non-string input into a string to handle NaN values.\n",
    "    Returns both processed entries and a flag indicating processing status.\n",
    "    \"\"\"\n",
    "    entry = str(entry)\n",
    "    flag = 0  # Default flag, 0 means no issues\n",
    "\n",
    "    if 'unidentified' in entry.lower():\n",
    "        return [(\"\", \"\", \"\")], 0  # Set flag to 1 if problematic\n",
    "\n",
    "    corporate_keywords = [\n",
    "        'Group', 'Company', 'Corporation', 'Incorporated', 'Inc.', 'LLC', 'Ltd.',\n",
    "        'and', '&', 'Architects', 'Architecture', 'Studio', 'Associates', 'Bureau',\n",
    "        'Office of Supervising Architect', 'Engineering', 'Construction', 'Consultants',\n",
    "        'Design', 'Partnership', 'Firm', 'PLLC', 'Office of',\n",
    "        'Skidmore Owings Merrill', 'Venturi Rauch & Scott Brown', 'architectural firm',\n",
    "        'architects', 'architecture', 'engineering', 'construction', 'design studio'\n",
    "    ]\n",
    "\n",
    "    entry = clean_name(entry)\n",
    "    primary_entities = [e.strip() for e in entry.split(';')]\n",
    "    processed_entries = []\n",
    "\n",
    "    for entity in primary_entities:\n",
    "        if '|' in entity:\n",
    "            name, role = [part.strip() for part in entity.split('|', 1)]\n",
    "        else:\n",
    "            name, role = entity, 'contributor'\n",
    "        \n",
    "        entity_type = 'corporate' if any(keyword.lower() in name.lower() for keyword in corporate_keywords) else 'person'\n",
    "        processed_entries.append((name, role, entity_type))\n",
    "    \n",
    "    return processed_entries, flag\n",
    "\n",
    "def expand_entries(row):\n",
    "    \"\"\"\n",
    "    Expand processed entries into separate columns within the same row.\n",
    "    Includes processing flag.\n",
    "    \"\"\"\n",
    "    expanded_data = {}\n",
    "    entries, flag = row['Processed']\n",
    "    for i, (name, role, entity_type) in enumerate(entries, start=1):\n",
    "        expanded_data[f'Primary archt/firm[{i}][Name]'] = name\n",
    "        expanded_data[f'Primary archt/firm[{i}][Role]'] = role\n",
    "        expanded_data[f'Primary archt/firm[{i}][Type]'] = entity_type\n",
    "    expanded_data['Primary archt/firm_flag'] = flag\n",
    "    return pd.Series(expanded_data)\n",
    "\n",
    "def process_and_save_csv(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Load the input CSV, process it with the latest rules, and save the outputs.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    df['Processed'] = df['Primary archt/firm'].apply(process_entry_v5)\n",
    "    expanded_df = df.apply(expand_entries, axis=1)\n",
    "    result_df = pd.concat([df, expanded_df], axis=1).drop(columns=['Processed'])\n",
    "    \n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"All data saved to {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = 'C:\\\\Users\\\\angel\\\\OneDrive\\\\Desktop\\\\Capstone\\\\Primary_archtfirm\\\\AAA_Proj_db_20231019-MSITM_Architech_input.csv'\n",
    "output_csv_path = 'C:\\\\Users\\\\angel\\\\OneDrive\\\\Desktop\\\\Capstone\\\\Primary_archtfirm\\\\AAA_Proj_db_20231019-MSITM_Architech_curated_output.csv'\n",
    "process_and_save_csv(input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_41236\\1466083765.py:66: DtypeWarning: Columns (35,43,47,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data saved to C:\\Users\\angel\\OneDrive\\Desktop\\Capstone\\Primary_archtfirm\\AAA_Proj_db_20231019-MSITM_Architech_curated_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"\n",
    "    Clean the name by removing unwanted characters and dates.\n",
    "    \"\"\"\n",
    "    name = re.sub('[^\\x00-\\x7F]+', '', name)  # Remove non-ASCII characters\n",
    "    name = re.sub('\\(\\d{4}-\\d{4}\\)|\\(\\d{4}\\)', '', name)  # Remove dates\n",
    "    return name.strip()\n",
    "\n",
    "def process_entry_v5(entry):\n",
    "    \"\"\"\n",
    "    Process each entry with enhanced rules for corporate identification and name cleaning.\n",
    "    Converts any non-string input into a string to handle NaN values.\n",
    "    Returns both processed entries and a flag indicating processing status.\n",
    "    \"\"\"\n",
    "    entry = str(entry)\n",
    "    flag = 0  # Default flag, 0 means no issues\n",
    "\n",
    "    if 'unidentified' in entry.lower():\n",
    "        return [(\"\", \"\", \"\")], 0  # Return empty values with flag set if problematic\n",
    "\n",
    "    corporate_keywords = [\n",
    "        'Group', 'Company', 'Corporation', 'Incorporated', 'Inc.', 'LLC', 'Ltd.',\n",
    "        'and', '&', 'Architects', 'Architecture', 'Studio', 'Associates', 'Bureau',\n",
    "        'Office of Supervising Architect', 'Engineering', 'Construction', 'Consultants',\n",
    "        'Design', 'Partnership', 'Firm', 'PLLC', 'Office of',\n",
    "        'Skidmore Owings Merrill', 'Venturi Rauch & Scott Brown', 'architectural firm',\n",
    "        'architects', 'architecture', 'engineering', 'construction', 'design studio'\n",
    "    ]\n",
    "\n",
    "    entry = clean_name(entry)\n",
    "    primary_entities = [e.strip() for e in entry.split(';')]\n",
    "    processed_entries = []\n",
    "\n",
    "    for entity in primary_entities:\n",
    "        if '|' in entity:\n",
    "            name, role = [part.strip() for part in entity.split('|', 1)]\n",
    "        else:\n",
    "            name, role = entity, 'contributor'\n",
    "        \n",
    "        entity_type = 'corporate' if any(keyword.lower() in name.lower() for keyword in corporate_keywords) else 'person'\n",
    "        processed_entries.append((name, role, entity_type))\n",
    "    \n",
    "    return processed_entries, flag\n",
    "\n",
    "def expand_entries(row):\n",
    "    \"\"\"\n",
    "    Expand processed entries into separate columns within the same row.\n",
    "    Includes processing flag.\n",
    "    \"\"\"\n",
    "    expanded_data = {}\n",
    "    entries, flag = row['Processed']\n",
    "    for i, (name, role, entity_type) in enumerate(entries, start=1):\n",
    "        expanded_data[f'Primary archt/firm[{i}][Name]'] = name\n",
    "        expanded_data[f'Primary archt/firm[{i}][Role]'] = role\n",
    "        expanded_data[f'Primary archt/firm[{i}][Type]'] = entity_type\n",
    "    expanded_data['Primary archt/firm_flag'] = flag\n",
    "    return pd.Series(expanded_data)\n",
    "\n",
    "def process_and_save_csv(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Load the input CSV, process it with the latest rules, and save the outputs.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    df['Processed'] = df['Primary archt/firm'].apply(process_entry_v5)\n",
    "    expanded_df = df.apply(expand_entries, axis=1)\n",
    "    \n",
    "    # Concatenate the original dataframe with the expanded dataframe\n",
    "    result_df = pd.concat([df, expanded_df], axis=1)\n",
    "    \n",
    "    # Drop the 'Processed' column which is no longer needed\n",
    "    result_df = result_df.drop(columns=['Processed'])\n",
    "\n",
    "    # Select only the desired columns\n",
    "    desired_columns = ['unique ID', 'Primary archt/firm'] + \\\n",
    "                      [col for col in result_df.columns if 'Primary archt/firm[' in col] + \\\n",
    "                      ['Primary archt/firm_flag']\n",
    "    result_df = result_df[desired_columns]\n",
    "    \n",
    "    # Save the result to a CSV file\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"All data saved to {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = 'C:\\\\Users\\\\angel\\\\OneDrive\\\\Desktop\\\\Capstone\\\\Primary_archtfirm\\\\AAA_Proj_db_20231019-MSITM_Architech_input.csv'\n",
    "output_csv_path = 'C:\\\\Users\\\\angel\\\\OneDrive\\\\Desktop\\\\Capstone\\\\Primary_archtfirm\\\\AAA_Proj_db_20231019-MSITM_Architech_curated_output.csv'\n",
    "process_and_save_csv(input_csv_path, output_csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
